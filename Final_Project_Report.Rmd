---
title: "Some Random Title Here"
#subtitle: "A subtitle"
thanks: "Paper submitted to complete the requirements of ENVSOCTY 4GA3 Applied Spatial Statistics; with additional edits by Antonio Paez for this version."
author:
- name: Farah Chin
  student_number: 400229991
- name: Talat Hakim
  student_number: 400315290
- name: Nathan Nadeau
  student_number: 400342430
- name: Cindia Dao-Vu
  student_number: 400319161
- name: Ifra Awan
  student_number:400261667 
- name: Oliver
  student_number: 
subject: "ENVSOCTY 4GA3"
abstract: "abstract goes here; some extra spiel added here"
keywords: "opioids, education, income, spatial analysis"
date: "4/6/2020"
output:
  pdf_document:
    # The project-template-default.tex file was minimally adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: project-template-default.tex
bibliography: [bibliography.bib, packages.bib]
always_allow_html: true
---

# Introduction

Words go here

More text for introduction
Add more words. For a review of land use regression models see @hoek2008review.

```{r}
rm(list = ls())
```

# Data

```{r message = FALSE}
# Load libraries for interpolation (might not need all of them)
library(deldir) # Delaunay Triangulation and Dirichlet (Voronoi) Tessellation
#library(isdas) # Companion Package for Book An Introduction to Spatial Data Analysis and Statistics
#library(plotly) # Create Interactive Web Graphics via 'plotly.js'
#library(spatstat) # Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
#library(spatstat.explore) # Exploratory Data Analysis for the 'spatstat' Family
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(sf)
library(cancensus) #retrieve Canadian Census Data
library(geojsonsf)
library(gstat)
library(stars)
library(readxl)

```

```{r}
# Load the air quality data
 
AQ_data <- read_excel('data/Air_Quality_Data.xlsx')
colnames(AQ_data)[4] <- c("P")
# 
# AQ_data <- mutate(AQ_data,
#                   X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
#                   XY = X * Y,
#                   Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

```{r}
summary(AQ_data)
```

```{r}
AQ_data.sf <- AQ_data |> 
  st_as_sf(coords = c("Lon", "Lat"), 
           remove = "FALSE") # Keep X and Y columns
```

```{r}
# function to determine the mean and sd of the prediction variance obtained with different kriging parameters
# observations: sf object containing the points to be interpolated (in this case the air pollution readings)
# polynomial terms need to have already been calculated
# targets: points that are being interpolated to (in this case the centroids)
# vgm_model: model of theoretical variogram (string)
# trend: what to use for the trend surface (can be Linear, Quadratic, Cubic, or None)
krig_pred_var <- function(observations, targets, vgm_model, trend = "None"){
  
  if(trend == "Linear"){
    trend_model <- P ~ X + Y
  } else if (trend == "Quadratic") {
    trend_model <- P ~ X2 + X + XY + Y + Y2
  } else if (trend == "Cubic") {
    trend_model <- P ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3
  } else {
    trend_model <- P ~ 1
  }
  
  variogram_v <- variogram(trend_model, 
                           data = observations)
  
  variogram_v.t <- fit.variogram(variogram_v, model = vgm(vgm_model))
  
  V.kriged <- krige(trend_model,
                    observations, 
                    targets, 
                    variogram_v.t)
  krig.list <- list(V.kriged, mean(V.kriged$var1.var), sd(V.kriged$var1.var))
  names(krig.list) <- c("Krig.output", "variance.mean", "variance.sd")
  return(krig.list)
}
```

```{r}
variogram_v <- variogram(P ~ 1, 
                           data = AQ_data.sf)

variogram_v.t <- fit.variogram(variogram_v, model = vgm("Exp", "Sph", "Gau"))
```


## Toronto Census Data 

Go to https://mountainmath.github.io/cancensus/index.html and follow instructions for setting up CensusMapping account and generating API key to retrieve data.

More info: https://mountainmath.github.io/cancensus/articles/cancensus.html

Set your cache to the data folder:
```{r}
cancensus::set_cancensus_cache_path(paste0(getwd(), "/data"),install = TRUE, overwrite = TRUE)
```


```{r}
#Getting median household income from cancensus:
# census tract level (GTA region)
torontoCTs.demographics.sf <- get_census(dataset='CA21', regions=list(CMA="35535"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6"), 
                         level='CT', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')

# dissemination area level (Toronto CSD)
torontoDAs.demographics.sf <- get_census(dataset='CA21', regions=list(CSD="3520005"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6"), 
                         level='DA', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')
```


```{r}
#Projecting:
st_transform(torontoDAs.demographics.sf, 32617)
st_crs(AQ_data.sf) <- st_crs(torontoDAs.demographics.sf)
```
```{r}
AQ_data.sf <- AQ_data.sf |>
  mutate(X = unlist(map(AQ_data.sf$geometry, 1)), 
         Y = unlist(map(AQ_data.sf$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```


```{r}
ggplot(torontoDAs.demographics.sf) + geom_sf() + geom_sf(data = AQ_data.sf, aes(size = P))
```
```{r}
# generating centroids
torontoDAs.centroids <- st_centroid(torontoDAs.demographics.sf)
```

```{r}
# Extract coordinates as separate 'X' and 'Y' columns
torontoDAs.centroids <- torontoDAs.centroids |>
  mutate(X = unlist(map(torontoDAs.centroids$geometry, 1)), 
         Y = unlist(map(torontoDAs.centroids$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

```{r}
#linear regression:
AQ.trend1 <- lm(formula = P ~ X + Y, 
                data = AQ_data.sf)
summary(AQ.trend1)

#quadratic regression:
AQ.trend2 <- lm(formula = P ~ X2 + X + XY + Y + Y2, 
                data = AQ_data.sf)
summary(AQ.trend2)

#cubic regression:
AQ.trend3 <- lm(formula = P ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3, 
                data = AQ_data.sf)
summary(AQ.trend3)

# looks like linear is the best model based on adjusted R2
```

```{r}
# use the linear regression model to predict values for AQ at each centroid location, then join these predictions to the centroids and plot
AQ.preds1 <- predict(AQ.trend1, 
                     newdata = torontoDAs.centroids, 
                     se.fit = TRUE, 
                     interval = "prediction", 
                     level = 0.95)

centroids.withPred <- mutate(torontoDAs.centroids, "pred" = AQ.preds1$fit[,1])

ggplot(centroids.withPred) + geom_sf(aes(colour = pred))
```

```{r}
AQ.res <- mutate(AQ_data.sf, "res" = AQ.trend1$residuals)

#interpolate the residuals using kpoint mean with k = 5

# kpoint.5 <- kpointmean(source_xy = AQ.res, 
#                        target_xy = torontoDAs.centroids, 
#                        z = res, 
#                        k = 5)
```



=======
>>>>>>> 1961d0d2742524a9f902b4c6b02647c83218166f
Words

# Methods

Describing some methods.

# Results and discussion

<!-- I think it would be a good idea to have a section with the discussion of the findings -->

Here we will discuss the results of the analysis.

# References

