---
title: "Some Random Title Here"
#subtitle: "A subtitle"
thanks: "Paper submitted to complete the requirements of ENVSOCTY 4GA3 Applied Spatial Statistics; with additional edits by Antonio Paez for this version."
author:
- name: Farah Chin
  student_number: 400229991
- name: Talat Hakim
  student_number: 400315290
- name: Nathan Nadeau
  student_number: 400342430
- name: Cindia Dao-Vu
  student_number: 400319161
- name: Ifra Awan
  student_number:400261667 
- name: Oliver
  student_number: 
subject: "ENVSOCTY 4GA3"
abstract: "abstract goes here; some extra spiel added here"
keywords: "opioids, education, income, spatial analysis"
date: "4/6/2020"
output:
  pdf_document:
    # The project-template-default.tex file was minimally adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: project-template-default.tex
bibliography: [bibliography.bib, packages.bib]
always_allow_html: true
---

# Introduction

Words go here

More text for introduction
Add more words. For a review of land use regression models see @hoek2008review.

```{r}
rm(list = ls())
```

# Data

```{r message = FALSE, warning = FALSE}
# Load libraries for interpolation (might not need all of them)
library(deldir) # Delaunay Triangulation and Dirichlet (Voronoi) Tessellation
library(isdas) # Companion Package for Book An Introduction to Spatial Data Analysis and Statistics
#library(plotly) # Create Interactive Web Graphics via 'plotly.js'
library(gstat)
library(spatstat) # Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
library(spatstat.explore) # Exploratory Data Analysis for the 'spatstat' Family
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(sf)
library(cancensus) #retrieve Canadian Census Data
library(geojsonsf)

library(stars)
library(readxl)
library(Metrics)
```

```{r}
# Load the air quality data
 
AQ_data <- read_excel('data/Air_Quality_Data.xlsx')
colnames(AQ_data)[4] <- c("P")
# 
# AQ_data <- mutate(AQ_data,
#                   X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
#                   XY = X * Y,
#                   Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

```{r}
summary(AQ_data)
```

```{r}
AQ_data.sf <- AQ_data |> 
  st_as_sf(coords = c("Lon", "Lat"), 
           remove = "FALSE") # Keep X and Y columns
```

```{r}
# function to determine the mean and sd of the prediction variance obtained with different kriging parameters
# observations: sf object containing the points to be interpolated (in this case the air pollution readings)
# polynomial terms need to have already been calculated
# targets: points that are being interpolated to (in this case the centroids)
# vgm_model: model of theoretical variogram (string)
# trend: what to use for the trend surface (can be Linear, Quadratic, Cubic, or None)
krig_pred_var <- function(observations, targets, vgm_model, trend = "None"){
  
  if(trend == "Linear"){
    trend_model <- P ~ X + Y
  } else if (trend == "Quadratic") {
    trend_model <- P ~ X2 + X + XY + Y + Y2
  } else if (trend == "Cubic") {
    trend_model <- P ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3
  } else {
    trend_model <- P ~ 1
  }
  
  variogram_v <- variogram(trend_model, 
                           data = observations)
  
  variogram_v.t <- fit.variogram(variogram_v, model = vgm(vgm_model))
  
  V.kriged <- krige(trend_model,
                    observations, 
                    targets, 
                    variogram_v.t)
  krig.list <- list(V.kriged, mean(V.kriged$var1.var), sd(V.kriged$var1.var))
  names(krig.list) <- c("Krig.output", "variance.mean", "variance.sd")
  return(krig.list)
}
```

```{r}
# Making air quality variogram
variogram_v <- variogram(P ~ 1, 
                           data = AQ_data.sf)

# Create best-fitting theoretical curve
variogram_v.t <- fit.variogram(variogram_v, model = vgm("Exp", "Sph", "Gau"))
variogram_v.t

gamma.t <- variogramLine(variogram_v.t, maxdist = 0.5)

# Plot semivariogram with fitted line
ggplot(data = variogram_v, 
       aes(x = dist,
           y = gamma)) + 
  geom_point() + 
  # Add labels to indicate the number of pairs of observations used
  # in the calculation of each point in the variogram
  geom_text(aes(label = np), 
            nudge_y = 10) +
  # Add theoretical semivariogram
  geom_line(data = gamma.t, 
            aes(x = dist, 
                y = gamma)) + 
  # Add labels to axes 
  xlab("Distance") + 
  ylab("Semivariance") 
```


## Toronto Census Data 

Go to https://mountainmath.github.io/cancensus/index.html and follow instructions for setting up CensusMapping account and generating API key to retrieve data.

More info: https://mountainmath.github.io/cancensus/articles/cancensus.html

Set your cache to the data folder:
```{r}
cancensus::set_cancensus_cache_path(paste0(getwd(), "/data"),install = TRUE, overwrite = TRUE)
```


```{r}
#Getting median household income from cancensus:

# census tract level (Toronto CSD)
torontoCTs.demographics.sf <- get_census(dataset='CA21', regions=list(CSD="3520005"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6",
                                   "median_age" = "v_CA21_389",
                                   "public_transit" = "v_CA21_7644",
                                   "automobile" = "v_CA21_7635",
                                   "walking" = "v_CA21_7647",
                                   "cycling" = "v_CA21_7650"), 
                         level='CT', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')

# dissemination area level (Toronto CSD)
torontoDAs.demographics.sf <- get_census(dataset='CA21', regions=list(CSD="3520005"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6",
                                   "median_age" = "v_CA21_389",
                                   "public_transit" = "v_CA21_7644",
                                   "automobile" = "v_CA21_7635",
                                   "walking" = "v_CA21_7647",
                                   "cycling" = "v_CA21_7650"), 
                         level='DA', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')
```


```{r echo = FALSE}
#Projecting:
st_transform(torontoDAs.demographics.sf, 32617)
#st_transform(torontoDAs.demographics.sf, 4269)
st_crs(torontoCTs.demographics.sf) <- st_crs(torontoDAs.demographics.sf)
st_crs(AQ_data.sf) <- st_crs(torontoDAs.demographics.sf)
```

```{r}
AQ_data.sf <- AQ_data.sf |>
  mutate(X = unlist(map(AQ_data.sf$geometry, 1)), 
         Y = unlist(map(AQ_data.sf$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```


```{r}
ggplot(torontoDAs.demographics.sf) + geom_sf() + geom_sf(data = AQ_data.sf, aes(size = P))
```


```{r}
# generating centroids
torontoDAs.centroids <- st_centroid(torontoDAs.demographics.sf)
torontoCTs.centroids <- st_centroid(torontoCTs.demographics.sf)
```

```{r}
# Extract coordinates as separate 'X' and 'Y' columns
torontoDAs.centroids <- torontoDAs.centroids |>
  mutate(X = unlist(map(torontoDAs.centroids$geometry, 1)), 
         Y = unlist(map(torontoDAs.centroids$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```



```{r warning = FALSE, message = FALSE}
# Trying kriging function again
toronto_int <- krig_pred_var(AQ_data.sf, torontoDAs.centroids, "Exp", "Cubic")
```
```{r}
# Get kriging results
V.kriged <- toronto_int$Krig.output
variance_mean <- toronto_int$variance.mean
variance_sd <- toronto_int$variance.sd

# Output results
V.kriged
variance_mean
variance_sd
```

```{r}
## IDW

to.bbox <- st_bbox(torontoCTs.demographics.sf)

# Get minimum and maximum values for aquifer data
min.X <- min(min(AQ_data$Lon), tobbox$xmin)
min.Y <- min(min(AQ_data$Lat), tobbox$ymin)
max.X <- max(max(AQ_data$Lon), tobbox$xmax)
max.Y <- max(max(AQ_data$Lat), tobbox$ymax)
x.range <- max.X - min.X
y.range <- max.Y - min.Y

# Define region of interest
AQ.bbox <- st_polygon(list(rbind(c(min.X, min.Y),
                                c(max.X, min.Y),
                                c(max.X, max.Y),
                                c(min.X, max.Y),
                                c(min.X, min.Y))))

AQ.owin <- as.owin(AQ.bbox)

# We can create a `ppp` object with the coordinates of the points
AQ_data.ppp <- as.ppp(X = AQ_data[,2:4], W = AQ.owin)
```

```{r}
summary(AQ_data.ppp)


plot(AQ_data.ppp)

```

Here we use Leave One Out cross validation to determine the optimal power: https://www.statology.org/leave-one-out-cross-validation/

See also:
https://rpubs.com/Dr_Gurpreet/interpolation_idw_R

```{r}
# CROSS VALIDATION

powers <- seq(0.001, 10, 0.01)
mse_result <- NULL
for(power in powers){
  P_idw <- idw(AQ_data.ppp, power=power, at="points")
  mse_result <- c(mse_result,
                  Metrics::mse(AQ_data.ppp$marks,P_idw))
}
optimal_power <- powers[which.min(mse_result)]
optimal_power
```

```{r}
P.idw_points <- idw(AQ_data.ppp, power = 0.471, at = "points")
P.idw_pixels <- idw(AQ_data.ppp, power = 0.471)
plot(P.idw_pixels, col=heat.colors(64))
#Metrics::mse(AQ_data.ppp$marks, z_p.idw1)
```

```{r}
# converting im object to a df
z.vec <- c(P.idw_pixels$v)
Y.vec <- rep(P.idw_pixels$yrow, 128)
X.vec <- rep(P.idw_pixels$xcol, each = 128)
P.idw.df <- data.frame("X" = X.vec, "Y" = Y.vec, "P" = z.vec)
P.idw.sf <- st_as_sf(P.idw.df, coords = c("X", "Y"))
st_crs(P.idw.sf) <- st_crs(torontoDAs.demographics.sf)


ggplot(P.idw.sf) + geom_sf(aes(colour = P)) + geom_sf(data = torontoDAs.demographics.sf)
ggplot(P.idw.sf) + geom_sf(aes(colour = P)) + geom_sf(data = torontoCTs.demographics.sf)
```

```{r}
toDAs.idw <- st_join(torontoDAs.centroids, P.idw.sf, join = st_nearest_feature)


ggplot(toDAs.idw) + geom_sf(aes(colour = P))

toCTs.idw <- na.omit(st_join(torontoCTs.demographics.sf, P.idw.sf, join = st_intersects))

ggplot(toCTs.idw) + geom_sf(aes(fill = P), colour = NA)
```


```{r message = FALSE}
# k.vals <- seq(1, 32)
# mse_result <- c()
# for(k in k.vals){
#   se.vec <- c()
#   for (i in 1:nrow(AQ_data.sf)){
#     train.sf <- AQ_data.sf[-i,]
#     test.sf <- AQ_data.sf[i, -4]
#     kpoint.k <- kpointmean(source_xy = train.sf,
#                        target_xy = test.sf,
#                        z = P,
#                        k = k)
# 
#     se.vec <- c(se.vec, (AQ_data.sf[i,]$P - kpoint.k$z)^2)
# 
#   }
#   mse <- mean(se.vec)
#   mse_result <- c(mse_result, mse)
# }
# optimal_k <- k.vals[which.min(mse_result)]
# optimal_k
```

```{r}
# k point means

kpoint.23 <- kpointmean(source_xy = AQ_data.sf, 
                       target_xy = torontoDAs.centroids, 
                       z = P, 
                       k = 23) |>
  rename(P = z)

ct.kpoint <- kpointmean(source_xy = AQ_data.sf, 
                       target_xy = torontoCTs.centroids, 
                       z = P, 
                       k = 23) |>
  rename(P = z)

ggplot() +
  geom_sf(data = kpoint.16, 
            aes(color = P)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)

ggplot() +
  geom_sf(data = ct.kpoint, 
            aes(color = P)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)
```



```{r}

# Add interpolation to DA centroids
toDA.centroids.kpoints <- mutate(torontoDAs.centroids, 
                             PM25 = kpoint.23$P)


toDA.kpoints <- mutate(torontoDAs.demographics.sf, 
                             PM25 = kpoint.23$P)

toCT.kpoints <- mutate(torontoCTs.demographics.sf, 
                             PM25 = ct.kpoint$P)

# Plot the centroids emissions
ggplot() +
  geom_sf(data = toDA.kpoints, 
            aes(fill = PM25), colour = NA) +
  scale_color_brewer(palette = "OrRd", 
                       direction = 1)

ggplot() +
  geom_sf(data = toCT.kpoints, 
            aes(fill = PM25), colour = NA) +
  scale_color_brewer(palette = "OrRd", 
                       direction = 1)

ggplot() +
  geom_sf(data = toDA.kpoints, 
            aes(color = PM25)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)

# Plot the populations
ggplot() +
  geom_sf(data = torontoDAs.demographics.sf, 
            aes(fill = Population)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)

# Plot the median HH income
ggplot() +
  geom_sf(data = torontoDAs.demographics.sf, 
            aes(fill = median_hh_income)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)

# Plot the centroids emissions
ggplot() +
  geom_sf(data = toDA.centroids.idw, 
            aes(color = automobile/Population)) +
  scale_color_distiller(palette = "OrRd", 
                       direction = 1)
```

```{r}
# torontoDAs.nb <- poly2nb(pl = toDAs.idw, queen = TRUE)
# torontoDAs.w <- nb2listw(torontoDAs.nb)

torontoCTs.nb <- poly2nb(pl = toCTs.idw)
torontoCTs.w <- nb2listw(torontoCTs.nb)

torontoCTs.nb.k <- poly2nb(pl = toCT.kpoints)
torontoCTs.k.w <- nb2listw(torontoCTs.nb.k, zero.policy = TRUE)
```


```{r}
moran.test(toCTs.idw$P, torontoCTs.w)

moran.test(toCT.kpoints$PM25, torontoCTs.k.w)

```

```{r}

# Linear regression function
# Ask for variable
linreg <- function(var, name){
  # Complete linear regression and output summary
  PM25_var_corr <- lm(formula = PM25 ~ var, data = toCT.kpoints)
  summary(PM25_var_corr)
  # Get variable name and title
  title <- paste("PM25 &", name, "- Linear Regression")
  # Plot results
  ggplot(data = toCT.kpoints, aes(x = var, y = PM25)) + 
    geom_point() + 
    geom_abline(slope = PM25_var_corr$coefficients[2], 
                intercept = PM25_var_corr$coefficients[1], 
                colour = "Blue", 
                linewidth = 1) + 
    labs(title = title) 
    
}
```

```{r}
linreg(toCT.kpoints$Population, "Population")
```


Words

# Methods

Describing some methods.

# Results and discussion

<!-- I think it would be a good idea to have a section with the discussion of the findings -->

Here we will discuss the results of the analysis.

# References

