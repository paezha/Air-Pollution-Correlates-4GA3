---
title: "Examining the relationship between socioeconomic variables and air pollution in Toronto"
#subtitle: "A subtitle"
thanks: "Paper submitted to complete the requirements of ENVSOCTY 4GA3 Applied Spatial Statistics; with additional edits by Antonio Paez for this version."
author:
- name: Farah Chin
  student_number: 400229991
- name: Talat Hakim
  student_number: 400315290
- name: Nathan Nadeau
  student_number: 400342430
- name: Cindia Dao-Vu
  student_number: 400319161
- name: Ifra Awan
  student_number: 400261667 
- name: Oliver Lawless
  student_number: 400271127
subject: "ENVSOCTY 4GA3"
abstract: "abstract goes here; some extra spiel added here"
keywords: "pollution, demographics, spatial analysis, transportation"
date: "4/19/2024"
output:
  pdf_document:
    # The project-template-default.tex file was minimally adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: project-template-default.tex
bibliography: [bibliography.bib, packages.bib]
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE)
```

# Introduction

This study analyzes the correlations between emission-based air pollution and socioeconomic spatial phenomena within the City of Toronto. It examines the predominant air quality factors responsible for the emission of fine particulate matter (PM2.5). The aim of this study is to address variations in air pollution statistics throughout the city. Generally, air quality tends to be worse in areas of lower socioeconomic status (SES) where people are forced to live near major sources of contamination such as major roads and factories. This paper contains a literature review to set a foundation for the analysis. Using air quality data from several sensors, we interpolated fine particulate matter concentrations across Toronto. Then, using data collected from the Canadian census we performed a linear regression analysis to determine the correlation between the air pollution of an area and the socioeconomic factors. Next, the statistics were summarized and interpreted to determine which socioeconomic factors are the most correlated with poor air quality. Finally, the paper concludes with some recommendations for future correlation analysis and steps the City of Toronto should take to manage air pollution.

# Background



```{r}
rm(list = ls())
```

# Data

```{r message = FALSE, warning = FALSE}
# Load libraries for interpolation (might not need all of them)
library(deldir) # Delaunay Triangulation and Dirichlet (Voronoi) Tessellation
library(isdas) # Companion Package for Book An Introduction to Spatial Data Analysis and Statistics
#library(plotly) # Create Interactive Web Graphics via 'plotly.js'
library(gstat) # for IDW
library(spatstat) # Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
library(spatstat.explore) # Exploratory Data Analysis for the 'spatstat' Family
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(sf)
library(cancensus) #retrieve Canadian Census Data
library(geojsonsf)
library(stars)
library(readxl) # for importing data from excel
library(Metrics) # for MSE function
library(patchwork) # for plotting multiple ggplots together
library(stringr) # for wrapping text in ggplot
library(gridExtra)
```

```{r}
# Load the air quality data
 
AQ_data <- read_excel('data/Air_Quality_Data.xlsx')
colnames(AQ_data)[4] <- c("P")
```

```{r}
AQ_data.sf <- AQ_data |> 
  st_as_sf(coords = c("Lon", "Lat"), 
           remove = "FALSE") # Keep X and Y columns
```

```{r, warning = FALSE}
# Making air quality variogram
variogram_v <- variogram(P ~ 1, 
                           data = AQ_data.sf)

# Create best-fitting theoretical curve
variogram_v.t <- fit.variogram(variogram_v, model = vgm("Exp", "Sph", "Gau"))

gamma.t <- variogramLine(variogram_v.t, maxdist = 0.5)

# Plot semivariogram with fitted line
ggplot(data = variogram_v, 
       aes(x = dist,
           y = gamma)) + 
  geom_point() + 
  # Add labels to indicate the number of pairs of observations used
  # in the calculation of each point in the variogram
  geom_text(aes(label = np), 
            nudge_y = 10) +
  # Add theoretical semivariogram
  geom_line(data = gamma.t, 
            aes(x = dist, 
                y = gamma)) + 
  # Add labels to axes 
  xlab("Distance") + 
  ylab("Semivariance") 
```


## Toronto Census Data 

Go to https://mountainmath.github.io/cancensus/index.html and follow instructions for setting up CensusMapping account and generating API key to retrieve data.

More info: https://mountainmath.github.io/cancensus/articles/cancensus.html

Set your cache to the data folder:
```{r}
cancensus::set_cancensus_cache_path(paste0(getwd(), "/data"),install = TRUE, overwrite = TRUE)
```


```{r}
# retrieving demographic data from cansensus for Toronto, at the dissemination area level

torontoDAs.demographics.sf <- get_census(dataset='CA21', regions=list(CSD="3520005"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "pop21" = "v_CA21_1",
                                   "population_density" = "v_CA21_6",
                                   "median_age" = "v_CA21_389",
                                   "public_transit" = "v_CA21_7644",
                                   "automobile" = "v_CA21_7635",
                                   "walking" = "v_CA21_7647",
                                   "cycling" = "v_CA21_7650",
                                   "minority_population" = "v_CA21_4875",
                                   "dwellings" = "v_CA21_434",
                                   "bachelors" = "v_CA21_5847"), 
                         level='DA', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')
```


```{r include = FALSE}
#Projecting first the Toronto shapefile, then using the same crs to project the air quality data:
st_transform(torontoDAs.demographics.sf, 32617)
st_crs(AQ_data.sf) <- st_crs(torontoDAs.demographics.sf)
```

```{r}
AQ_data.sf <- AQ_data.sf |>
  mutate(X = unlist(map(AQ_data.sf$geometry, 1)), 
         Y = unlist(map(AQ_data.sf$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```


```{r}
ggplot(torontoDAs.demographics.sf) + geom_sf() + geom_sf(data = AQ_data.sf, aes(size = P)) + labs(size = "Particulate Matter reading")
```

```{r}
# generating centroids
torontoDAs.centroids <- st_centroid(torontoDAs.demographics.sf)
```


```{r}
## IDW

to.bbox <- st_bbox(torontoDAs.demographics.sf)

# Get minimum and maximum values for aquifer data
min.X <- min(min(AQ_data$Lon), to.bbox$xmin)
min.Y <- min(min(AQ_data$Lat), to.bbox$ymin)
max.X <- max(max(AQ_data$Lon), to.bbox$xmax)
max.Y <- max(max(AQ_data$Lat), to.bbox$ymax)
x.range <- max.X - min.X
y.range <- max.Y - min.Y

# Define region of interest
AQ.bbox <- st_polygon(list(rbind(c(min.X, min.Y),
                                c(max.X, min.Y),
                                c(max.X, max.Y),
                                c(min.X, max.Y),
                                c(min.X, min.Y))))

AQ.owin <- as.owin(AQ.bbox)

# We can create a `ppp` object with the coordinates of the points
AQ_data.ppp <- as.ppp(X = AQ_data[,2:4], W = AQ.owin)
```

Here we use Leave One Out cross validation to determine the optimal power: https://www.statology.org/leave-one-out-cross-validation/

See also:
https://rpubs.com/Dr_Gurpreet/interpolation_idw_R

```{r}
# CROSS VALIDATION

powers <- seq(0.001, 10, 0.01)
mse_result <- NULL
for(power in powers){
  P_idw <- idw(AQ_data.ppp, power=power, at="points")
  mse_result <- c(mse_result,
                  Metrics::mse(AQ_data.ppp$marks,P_idw))
}
optimal_power <- powers[which.min(mse_result)]
optimal_power
```

```{r}
P.idw_points <- idw(AQ_data.ppp, power = 0.471, at = "points")
P.idw_pixels <- idw(AQ_data.ppp, power = 0.471)
plot(P.idw_pixels, col=heat.colors(64))
#Metrics::mse(AQ_data.ppp$marks, z_p.idw1)
```

```{r}
# converting im object to a df
z.vec <- c(P.idw_pixels$v)
Y.vec <- rep(P.idw_pixels$yrow, 128)
X.vec <- rep(P.idw_pixels$xcol, each = 128)
P.idw.df <- data.frame("X" = X.vec, "Y" = Y.vec, "P" = z.vec)
P.idw.sf <- st_as_sf(P.idw.df, coords = c("X", "Y"))
st_crs(P.idw.sf) <- st_crs(torontoDAs.demographics.sf)


ggplot(P.idw.sf) + geom_sf(aes(colour = P)) + geom_sf(data = torontoDAs.demographics.sf)
```

```{r}
# use spatial join to add the PM25 value of the nearest pixel to each centroid
toDAs.idw <- st_join(torontoDAs.centroids, P.idw.sf, join = st_nearest_feature)

toDAs.idw.poly <- mutate(torontoDAs.demographics.sf, P = toDAs.idw$P)
toDAs.NAomit <- na.omit(toDAs.idw.poly)
ggplot(toDAs.idw.poly) + geom_sf(aes(fill = P), colour = NA) + labs(title = "Interpolated Particulate Matter Readings", fill = "Interpolated Particulate Matter value") + scale_fill_viridis_c(option = "magma")
```


```{r message = FALSE}
# k.vals <- seq(1, 32)
# mse_result <- c()
# for(k in k.vals){
#   se.vec <- c()
#   for (i in 1:nrow(AQ_data.sf)){
#     train.sf <- AQ_data.sf[-i,]
#     test.sf <- AQ_data.sf[i, -4]
#     kpoint.k <- kpointmean(source_xy = train.sf,
#                        target_xy = test.sf,
#                        z = P,
#                        k = k)
# 
#     se.vec <- c(se.vec, (AQ_data.sf[i,]$P - kpoint.k$z)^2)
# 
#   }
#   mse <- mean(se.vec)
#   mse_result <- c(mse_result, mse)
# }
# optimal_k <- k.vals[which.min(mse_result)]
# optimal_k
```

```{r}
# function for generating chloropleth maps
chloropleth.plot <- function(var, var.name){
  ggplot(data = toDAs.idw.poly) +
  geom_sf(aes(fill = var), colour = NA) + 
    scale_fill_viridis_c(option = "magma") + labs(title = str_wrap(var.name, 30), fill = str_wrap(var.name, 30))
}
```

```{r}
pop_den.chl <- chloropleth.plot(toDAs.idw.poly$pop21/toDAs.idw.poly$`Area (sq km)`, "Population density")
minority.chl <- chloropleth.plot(toDAs.idw.poly$minority_population/toDAs.idw.poly$pop21, "Proportion visible minority")
med_inc.chl <- chloropleth.plot(toDAs.idw.poly$median_hh_income, "Median household income")
med_age.chl <- chloropleth.plot(toDAs.idw.poly$median_age, "Median age")
automobile.chl <- chloropleth.plot(toDAs.idw.poly$automobile/toDAs.idw.poly$pop21, "Proportion commuting via automobile")
public_transit.chl <- chloropleth.plot(toDAs.idw.poly$public_transit/toDAs.idw.poly$pop21, "Proportion commuting via public transit")
walking.chl <- chloropleth.plot(toDAs.idw.poly$walking/toDAs.idw.poly$pop21, "Proportion commuting via walking")
cycling.chl <- chloropleth.plot(toDAs.idw.poly$cycling/toDAs.idw.poly$pop21, "Proportion commuting via cycling")
```

```{r}
(pop_den.chl | minority.chl) /
  (med_inc.chl | med_age.chl)

(automobile.chl | public_transit.chl) /
  (walking.chl | cycling.chl)
```

```{r}
grid.arrange(pop_den.chl, minority.chl, med_inc.chl, med_age.chl, nrow = 2)
```



```{r}
torontoDAsna.nb <- poly2nb(pl = toDAs.NAomit, queen = TRUE)
torontoDAsna.w <- nb2listw(torontoDAsna.nb, zero.policy = TRUE)
```



```{r}
# function for returning moran's I and p value
moran.test.sum <- function(var){
  m <- moran.test(var, torontoDAsna.w)
  return(c("I" = m$estimate[1], "p.val" = m$p.value))
}
```

```{r}
moran.vec <- c("Population density", moran.test.sum(toDAs.NAomit$population_density),
               "Median household income", moran.test.sum(toDAs.NAomit$median_hh_income),
               "Median age", moran.test.sum(toDAs.NAomit$median_age),
               "Proportion visible minority", moran.test.sum(toDAs.NAomit$minority_population/toDAs.NAomit$pop21),
               "Proportion commuting via automobile", moran.test.sum(toDAs.NAomit$automobile/toDAs.NAomit$pop21),
               "Proportion commuting via public transportation", moran.test.sum(toDAs.NAomit$public_transit/toDAs.NAomit$pop21),
               "Proportion commuting via walking", moran.test.sum(toDAs.NAomit$walking/toDAs.NAomit$pop21),
               "Proportion commuting via cycling", moran.test.sum(toDAs.NAomit$cycling/toDAs.NAomit$pop21))

moran.mat <- matrix(moran.vec, ncol = 3, byrow = TRUE)

colnames(moran.mat) <- c("Variable", "Moran's Statistic (I)", "p-value")

moran.mat
```


```{r}

# Linear regression function
# Ask for variable
linreg <- function(var, name){
  # Complete linear regression and output summary
  PM25_var_corr <- lm(formula = P ~ var, data = toDAs.NAomit)
  #summary(PM25_var_corr)
  # Get variable name and title
  #title <- paste("PM25 &", name, "- Linear Regression")
  title <- paste(name, 
                 "; AR2 =", round(summary(PM25_var_corr)$r.squared, 2))
  # Plot results
  ggplot(data = toDAs.NAomit, aes(x = var, y = P)) + 
    geom_point() + 
    geom_abline(slope = PM25_var_corr$coefficients[2], 
                intercept = PM25_var_corr$coefficients[1], 
                colour = "Blue", 
                linewidth = 1) + 
    labs(title = str_wrap(title, 30)) +
    xlab(name) + ylab("Particulate Matter")
    
}
```

```{r}
Pop_den.lm <- linreg(toDAs.NAomit$population_density, "Population density")
med_inc.lm <- linreg(toDAs.NAomit$median_hh_income, "Median household income")
med_age.lm <- linreg(toDAs.NAomit$median_age, "Median Age")
automobile.lm <- linreg(toDAs.NAomit$automobile/toDAs.NAomit$pop21, "Proportion commuting via automobile")
public_transit.lm <- linreg(toDAs.NAomit$public_transit/toDAs.NAomit$pop21, "Proportion commuting via public transit")
walking.lm <- linreg(toDAs.NAomit$walking/toDAs.NAomit$pop21, "Proportion commuting via walking")
cycling.lm <- linreg(toDAs.NAomit$cycling/toDAs.NAomit$pop21, "Proportion commuting via cycling")
minority.lm <- linreg(toDAs.NAomit$minority_population/toDAs.NAomit$pop21, "Proportion of visible  minorities")
dwellings.lm <- linreg(toDAs.NAomit$dwellings/toDAs.NAomit$`Area (sq km)`, "Dwellings density")
bachelors.lm <- linreg(toDAs.NAomit$bachelors/toDAs.NAomit$pop21, "Proportion with at least a bachelor's")
```

```{r}
(Pop_den.lm | med_inc.lm) /
  (med_age.lm | minority.lm)

(automobile.lm | public_transit.lm) /
(walking.lm | cycling.lm )
```

```{r}
toDAs.NAomit$minority_proportion <- toDAs.NAomit$minority_population/toDAs.NAomit$pop21
toDAs.NAomit$automobile_prop <- toDAs.NAomit$automobile/toDAs.NAomit$pop21
toDAs.NAomit$public_tr_prop <- toDAs.NAomit$public_transit/toDAs.NAomit$pop21
toDAs.NAomit$walking_prop <- toDAs.NAomit$walking/toDAs.NAomit$pop21
toDAs.NAomit$cycling_prop <- toDAs.NAomit$cycling/toDAs.NAomit$pop21
```

To further investigate the relative importance of each covariate, linear regression was performed using all eight covariates. It is also possible that this might result in a better fit than models involving only single covariates.

```{r}
lm.all_vars <- lm(formula = P ~ population_density + 
                    minority_proportion + 
                    median_age +
                    median_hh_income +
                    automobile_prop +
                    public_tr_prop + 
                    walking_prop +
                    cycling_prop, 
                  data = toDAs.NAomit)
```

```{r}
summary(lm.all_vars)
```

```{r}
# finding best predictors

lm.best_vars <- lm(formula = P ~ minority_proportion + 
                    median_age +
                    automobile_prop +
                    walking_prop +
                    cycling_prop,
                   data = toDAs.NAomit)

summary(lm.best_vars)
```
Despite using only the covariates with the lowest p-value, the Adjusted R-squared value decreases slightly and the residual standard error increases. This indicates that reducing the number of variables in the model does not in fact improve the fit. Therefore, for general prediction it might be best to use all 8 of the selected variables. 

```{r}
toDAs.res <- mutate(toDAs.NAomit, "res" = lm.all_vars$residuals)
toDAs.res.sign <- toDAs.res$res > 0 

ggplot(toDAs.res) + geom_sf(aes(fill = toDAs.res.sign))
```



Words

# Methods

Describing some methods.

# Results and discussion

<!-- I think it would be a good idea to have a section with the discussion of the findings -->

Here we will discuss the results of the analysis.

# References

