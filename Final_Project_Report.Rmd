---
title: "Some Random Title Here"
#subtitle: "A subtitle"
thanks: "Paper submitted to complete the requirements of ENVSOCTY 4GA3 Applied Spatial Statistics; with additional edits by Antonio Paez for this version."
author:
- name: Farah Chin
  student_number: 400229991
- name: Talat Hakim
  student_number: 400315290
- name: Nathan Nadeau
  student_number: 400342430
- name: Cindia Dao-Vu
  student_number: 400319161
- name: Ifra Awan
  student_number:400261667 
- name: Oliver
  student_number: 
subject: "ENVSOCTY 4GA3"
abstract: "abstract goes here; some extra spiel added here"
keywords: "opioids, education, income, spatial analysis"
date: "4/6/2020"
output:
  pdf_document:
    # The project-template-default.tex file was minimally adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: project-template-default.tex
bibliography: [bibliography.bib, packages.bib]
always_allow_html: true
---

# Introduction

Words go here

More text for introduction
Add more words. For a review of land use regression models see @hoek2008review.

```{r}
rm(list = ls())
```

# Data

```{r}
# Load libraries for interpolation (might not need all of them)
library(deldir) # Delaunay Triangulation and Dirichlet (Voronoi) Tessellation
#library(isdas) # Companion Package for Book An Introduction to Spatial Data Analysis and Statistics
library(plotly) # Create Interactive Web Graphics via 'plotly.js'
#library(spatstat) # Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
#library(spatstat.explore) # Exploratory Data Analysis for the 'spatstat' Family
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(sf)
library(cancensus) #retrieve Canadian Census Data
library(geojsonsf)
library(gstat)
library(stars)
library(readxl)

```

```{r}
# Load the air quality data
 
AQ_data=read_excel('data/Air_Quality_Data.xlsx')

AQ_data
```

```{r}
summary(AQ_data)
```

```{r}
AQ_plot <- ggplot(data = AQ_data, 
              aes (x = Lon, y = Lat, size = PM25Avg)) +
  geom_point(alpha = 0.5) + 
  scale_color_distiller(palette = "OrRd",
                        direction = 1)

AQ_plot
```

```{r}
AQ_data.sf <- AQ_data |> 
  st_as_sf(coords = c("Lon", "Lat"))
```

```{r}
vpolygons <- do.call(c, st_geometry(AQ_data.sf)) |> 
  st_voronoi() |> 
  st_collection_extract()
```

```{r}
ggplot(vpolygons) + 
  geom_sf(fill = NA)
```

```{r}
AQ_data.v <- AQ_data.sf
AQ_data.v$geometry <- vpolygons[unlist(st_intersects(AQ_data.sf, vpolygons))] 
```

```{r}
ggplot(AQ_data.v) + 
  geom_sf(aes(fill = PM25Avg)) +
  scale_fill_distiller(palette = "OrRd", 
                       direction = 1)
```

```{r}
# function to determine the mean and sd of the prediction variance obtained with different kriging parameters
# observations: sf object containing the points to be interpolated (in this case the air pollution readings)
# polynomial terms need to have already been calculated
# targets: points that are being interpolated to (in this case the centroids)
# vgm_model: model of theoretical variogram (string)
# trend: what to use for the trend surface (can be Linear, Quadratic, Cubic, or None)
krig_pred_var <- function(observations, targets, vgm_model, trend = "None"){
  
  if(trend == "Linear"){
    trend_model <- V ~ X + Y
  } else if (trend == "Quadratic") {
    trend_model <- V ~ X2 + X + XY + Y + Y2
  } else if (trend == "Cubic") {
    trend_model <- V ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3
  } else {
    trend_model <- V ~ 1
  }
  
  variogram_v <- variogram(trend_model, 
                           data = observations)
  
  variogram_v.t <- fit.variogram(variogram_v, model = vgm(vgm_model))
  
  V.kriged <- krige(trend_model,
                    observations, 
                    targets, 
                    variogram_v.t)
  krig.list <- list(V.kriged, mean(V.kriged$var1.var), sd(V.kriged$var1.var))
  names(krig.list) <- c("Krig.output", "variance.mean", "variance.sd")
  return(krig.list)
}
```

## Toronto Census Data 

Go to https://mountainmath.github.io/cancensus/index.html and follow instructions for setting up CensusMapping account and generating API key to retrieve data.

More info: https://mountainmath.github.io/cancensus/articles/cancensus.html

```{r}
# regions available for 2016 census
list_census_regions("CA21")

#Toronto: CMA
```
Set your cache to the data folder:
```{r}
cancensus::set_cancensus_cache_path(paste0(getwd(), "/data"),install = TRUE, overwrite = TRUE)
```


```{r}
#Getting median household income from cancensus:
# census tract level 
torontoCTs.demographics.sf <- get_census(dataset='CA21', regions=list(CMA="35535"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6"), 
                         level='CT', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')

# dissemination area level
torontoDAs.demographics.sf <- get_census(dataset='CA21', regions=list(CMA="35535"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6"), 
                         level='DA', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')
```


```{r}
#Projecting:
st_transform(torontoDAs.demographics.sf, 32617)
st_crs(AQ_data.sf) <- st_crs(torontoDAs.demographics.sf)
```

```{r}
ggplot(torontoDAs.demographics.sf) + geom_sf() + geom_sf(data = AQ_data.sf, aes(size = PM25Avg))
```
```{r}
# generating centroids
torontoDAs.centroids <- st_centroid(torontoDAs.demographics.sf)
```

=======
>>>>>>> 1961d0d2742524a9f902b4c6b02647c83218166f
Words

# Methods

Describing some methods.

# Results and discussion

<!-- I think it would be a good idea to have a section with the discussion of the findings -->

Here we will discuss the results of the analysis.

# References

