---
title: "Some Random Title Here"
#subtitle: "A subtitle"
thanks: "Paper submitted to complete the requirements of ENVSOCTY 4GA3 Applied Spatial Statistics; with additional edits by Antonio Paez for this version."
author:
- name: Farah Chin
  student_number: 400229991
- name: Talat Hakim
  student_number: 400315290
- name: Nathan Nadeau
  student_number: 400342430
- name: Cindia Dao-Vu
  student_number: 400319161
- name: Ifra Awan
  student_number:400261667 
- name: Oliver
  student_number: 
subject: "ENVSOCTY 4GA3"
abstract: "abstract goes here; some extra spiel added here"
keywords: "opioids, education, income, spatial analysis"
date: "4/6/2020"
output:
  pdf_document:
    # The project-template-default.tex file was minimally adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: project-template-default.tex
bibliography: [bibliography.bib, packages.bib]
always_allow_html: true
---

# Introduction

Words go here

More text for introduction
Add more words. For a review of land use regression models see @hoek2008review.

```{r}
rm(list = ls())
```

# Data

```{r message = FALSE}
# Load libraries for interpolation (might not need all of them)
library(deldir) # Delaunay Triangulation and Dirichlet (Voronoi) Tessellation
library(isdas) # Companion Package for Book An Introduction to Spatial Data Analysis and Statistics
#library(plotly) # Create Interactive Web Graphics via 'plotly.js'
library(spatstat) # Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
library(spatstat.explore) # Exploratory Data Analysis for the 'spatstat' Family
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(sf)
library(cancensus) #retrieve Canadian Census Data
library(geojsonsf)
library(gstat)
library(stars)
library(readxl)
library(Metrics)
```

```{r}
# Load the air quality data
 
AQ_data <- read_excel('data/Air_Quality_Data.xlsx')
colnames(AQ_data)[4] <- c("P")
# 
# AQ_data <- mutate(AQ_data,
#                   X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
#                   XY = X * Y,
#                   Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

```{r}
summary(AQ_data)
```

```{r}
AQ_data.sf <- AQ_data |> 
  st_as_sf(coords = c("Lon", "Lat"), 
           remove = "FALSE") # Keep X and Y columns
```

```{r}
# function to determine the mean and sd of the prediction variance obtained with different kriging parameters
# observations: sf object containing the points to be interpolated (in this case the air pollution readings)
# polynomial terms need to have already been calculated
# targets: points that are being interpolated to (in this case the centroids)
# vgm_model: model of theoretical variogram (string)
# trend: what to use for the trend surface (can be Linear, Quadratic, Cubic, or None)
krig_pred_var <- function(observations, targets, vgm_model, trend = "None"){
  
  if(trend == "Linear"){
    trend_model <- P ~ X + Y
  } else if (trend == "Quadratic") {
    trend_model <- P ~ X2 + X + XY + Y + Y2
  } else if (trend == "Cubic") {
    trend_model <- P ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3
  } else {
    trend_model <- P ~ 1
  }
  
  variogram_v <- variogram(trend_model, 
                           data = observations)
  
  variogram_v.t <- fit.variogram(variogram_v, model = vgm(vgm_model))
  
  V.kriged <- krige(trend_model,
                    observations, 
                    targets, 
                    variogram_v.t)
  krig.list <- list(V.kriged, mean(V.kriged$var1.var), sd(V.kriged$var1.var))
  names(krig.list) <- c("Krig.output", "variance.mean", "variance.sd")
  return(krig.list)
}
```

```{r}
# Making air quality variogram
variogram_v <- variogram(P ~ 1, 
                           data = AQ_data.sf)

# Create best-fitting theoretical curve
variogram_v.t <- fit.variogram(variogram_v, model = vgm("Exp", "Sph", "Gau"))
variogram_v.t

gamma.t <- variogramLine(variogram_v.t, maxdist = 0.5)

# Plot semivariogram with fitted line
ggplot(data = variogram_v, 
       aes(x = dist,
           y = gamma)) + 
  geom_point() + 
  # Add labels to indicate the number of pairs of observations used
  # in the calculation of each point in the variogram
  geom_text(aes(label = np), 
            nudge_y = 10) +
  # Add theoretical semivariogram
  geom_line(data = gamma.t, 
            aes(x = dist, 
                y = gamma)) + 
  # Add labels to axes 
  xlab("Distance") + 
  ylab("Semivariance") 
```


## Toronto Census Data 

Go to https://mountainmath.github.io/cancensus/index.html and follow instructions for setting up CensusMapping account and generating API key to retrieve data.

More info: https://mountainmath.github.io/cancensus/articles/cancensus.html

Set your cache to the data folder:
```{r}
cancensus::set_cancensus_cache_path(paste0(getwd(), "/data"),install = TRUE, overwrite = TRUE)
```


```{r}
#Getting median household income from cancensus:

# dissemination area level (Toronto CSD)
torontoDAs.demographics.sf <- get_census(dataset='CA21', regions=list(CSD="3520005"),
                         vectors=c("median_hh_income"="v_CA21_906",
                                   "population_density" = "v_CA21_6",
                                   "median_age" = "v_CA21_389",
                                   "public_transit" = "v_CA21_7644",
                                   "automobile" = "v_CA21_7635",
                                   "walking" = "v_CA21_7647",
                                   "cycling" = "v_CA21_7650"), 
                         level='DA', quiet = TRUE, 
                         geo_format = 'sf', labels = 'short')
```


```{r}
#Projecting:
st_transform(torontoDAs.demographics.sf, 32617)
#st_transform(torontoDAs.demographics.sf, 4269)
st_crs(AQ_data.sf) <- st_crs(torontoDAs.demographics.sf)
```

```{r}
AQ_data.sf <- AQ_data.sf |>
  mutate(X = unlist(map(AQ_data.sf$geometry, 1)), 
         Y = unlist(map(AQ_data.sf$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```


```{r}
ggplot(torontoDAs.demographics.sf) + geom_sf() + geom_sf(data = AQ_data.sf, aes(size = P))
```


```{r}
# generating centroids
torontoDAs.centroids <- st_centroid(torontoDAs.demographics.sf)
```

```{r}
# Extract coordinates as separate 'X' and 'Y' columns
torontoDAs.centroids <- torontoDAs.centroids |>
  mutate(X = unlist(map(torontoDAs.centroids$geometry, 1)), 
         Y = unlist(map(torontoDAs.centroids$geometry, 2))) |>
  # Add polynomials
  mutate(X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
         XY = X * Y, 
         Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

```{r}
#linear regression:
AQ.trend1 <- lm(formula = P ~ X + Y, 
                data = AQ_data.sf)
summary(AQ.trend1)

#quadratic regression:
AQ.trend2 <- lm(formula = P ~ X2 + X + XY + Y + Y2, 
                data = AQ_data.sf)
summary(AQ.trend2)

#cubic regression:
AQ.trend3 <- lm(formula = P ~ X3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3, 
                data = AQ_data.sf)
summary(AQ.trend3)

# looks like linear is the best model based on adjusted R2
```

```{r}
# use the linear regression model to predict values for AQ at each centroid location, then join these predictions to the centroids and plot
AQ.preds1 <- predict(AQ.trend1, 
                     newdata = torontoDAs.centroids, 
                     se.fit = TRUE, 
                     interval = "prediction", 
                     level = 0.95)

centroids.withPred <- mutate(torontoDAs.centroids, "pred" = AQ.preds1$fit[,1])

ggplot(centroids.withPred) + geom_sf(aes(colour = pred)) 
```

```{r}
AQ.res <- mutate(AQ_data.sf, "res" = AQ.trend1$residuals)
AQ.res$resSign <- AQ.res$res > 0
ggplot(AQ.res) + geom_sf(aes(colour = resSign))

#interpolate the residuals using kpoint mean with k = 5

# kpoint.5 <- kpointmean(source_xy = AQ.res, 
#                        target_xy = torontoDAs.centroids, 
#                        z = res, 
#                        k = 5)
```


```{r warning = FALSE, message = FALSE}
# Trying kriging function again
toronto_int <- krig_pred_var(AQ_data.sf, torontoDAs.centroids, "Exp", "Cubic")
```
```{r}
# Get kriging results
V.kriged <- toronto_int$Krig.output
variance_mean <- toronto_int$variance.mean
variance_sd <- toronto_int$variance.sd

# Output results
V.kriged
variance_mean
variance_sd
```
```{r}
## IDW

# Get minimum and maximum values for aquifer data
toronto.bbox <- st_bbox(torontoDAs.demographics.sf)

min.X <- min(min(AQ_data$Lon), toronto.bbox$xmin)
min.Y <- min(min(AQ_data$Lat), toronto.bbox$ymin)
max.X <- max(max(AQ_data$Lon), toronto.bbox$xmax)
max.Y <- max(max(AQ_data$Lat), toronto.bbox$ymax)


x.range <- max.X - min.X
y.range <- max.Y - min.Y

# Define region of interest
AQ.bbox <- st_polygon(list(rbind(c(min.X, min.Y),
                                c(max.X, min.Y),
                                c(max.X, max.Y),
                                c(min.X, max.Y),
                                c(min.X, min.Y))))

AQ.owin <- as.owin(AQ.bbox)

# We can create a `ppp` object with the coordinates of the points
AQ_data.ppp <- as.ppp(X = AQ_data[,2:4], W = AQ.owin)
```

```{r}
summary(AQ_data.ppp)


plot(AQ_data.ppp)

```


```{r}

idw.rmse <- function(ntry, power){
  rmse.vec <- c()
  for (i in 1:ntry){
    train <- sample(1:nrow(AQ_data), 22)
    train.idw <- idw(AQ_data.ppp[train,], power = power)
    #p.idw <- idw(AQ_data.ppp, power = i, at = "points")
    test.points <- AQ_data[-train,]
    e.vec <- c()
    for (j in 1:nrow(test.points)){
      x.val <- test.points$Lon[j]
      y.val <- test.points$Lat[j]
      x.matIndex <- floor(((x.val - min.X)/x.range)*128)
      y.matIndex <- floor(((y.val - min.Y)/y.range)*128)
      idw.val <- train.idw[y.matIndex, x.matIndex]
      se <- (test.points$P[j] - idw.val)^2
      e.vec <- append(e.vec, se)
    }
    #mse <- Metrics::mse(AQ_data.ppp$marks, p.idw)
    rmse <- sqrt(mean(e.vec))
    rmse.vec <- append(rmse.vec, rmse)
  }
  return(mean(rmse))
}

```

```{r}
#idw.rmse(100, 0)
set.seed(2024)
power <- seq(0, 1, 0.1)

for (i in power){
  print(paste("power:", i, "rmse:", idw.rmse(100, i)))
  #print(i)
}
```

Here we use Leave One Out cross validation to determine the optimal power: https://www.statology.org/leave-one-out-cross-validation/

See also:
https://rpubs.com/Dr_Gurpreet/interpolation_idw_R

```{r}
# CROSS VALIDATION

powers <- seq(0.001, 10, 0.01)
mse_result <- NULL
for(power in powers){
  P_idw <- idw(AQ_data.ppp, power=power, at="points")
  mse_result <- c(mse_result,
                  Metrics::mse(AQ_data.ppp$marks,P_idw))
}
optimal_power <- powers[which.min(mse_result)]
optimal_power
```

```{r}
P.idw_points <- idw(AQ_data.ppp, power = 0.471, at = "points")
P.idw_pixels <- idw(AQ_data.ppp, power = 0.471)
plot(P.idw_pixels, col=heat.colors(64))
#Metrics::mse(AQ_data.ppp$marks, z_p.idw1)
```

```{r}
# converting im object to a df
z.vec <- c(P.idw_pixels$v)
Y.vec <- rep(P.idw_pixels$yrow, 128)
X.vec <- rep(P.idw_pixels$xcol, each = 128)
P.idw.df <- data.frame("X" = X.vec, "Y" = Y.vec, "P" = z.vec)
P.idw.sf <- st_as_sf(P.idw.df, coords = c("X", "Y"))
st_crs(P.idw.sf) <- st_crs(torontoDAs.demographics.sf)

ggplot(P.idw.sf) + geom_sf(aes(colour = P)) + geom_sf(data = torontoDAs.demographics.sf) 
```

```{r}
toronto.DAs.P <- st_join(torontoDAs.demographics.sf, P.idw.sf, join = st_contains)

ggplot(toronto.DAs.P) + geom_sf(aes(fill = P))
```


```{r}
# k point means

kpoint.10 <- kpointmean(source_xy = AQ_data.sf, 
                       target_xy = torontoDAs.centroids, 
                       z = P, 
                       k = 10) |>
  rename(P = z)

# ggplot() +
#   geom_sf(data = kpoint.10, 
#             aes(color = P)) +
#   scale_color_distiller(palette = "OrRd", 
#                        direction = 1)
```
some more code

SEPARATOR

Words

# Methods

Describing some methods.

# Results and discussion

<!-- I think it would be a good idea to have a section with the discussion of the findings -->

Here we will discuss the results of the analysis.

# References

